import os
import re
import hashlib
from typing import List, Dict, Tuple, Generator

import numpy as np
import pandas as pd
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ==== å›ºå®šè¨­å®š =====================================================
CSV_FILE = "dance_school.csv"          # å›ºå®šã®å‚è€ƒè³‡æ–™CSV
ENCODING = "utf-8"
INDEX_COLS = ["name", "overview", "access"]
CHUNK_SIZE = 700
CHUNK_OVERLAP = 150
EMBED_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"
TOP_K = 6
MIN_TOP_SIM = 0.20
USE_HISTORY_TURNS = 2
MAX_CONTEXT_CHARS = 4500
STREAM_ANSWER = True   # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
# ==================================================================

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

st.set_page_config(page_title="Dance School Guide Chat | ãƒ€ãƒ³ã‚¹ã‚¹ã‚¯ãƒ¼ãƒ«æ¡ˆå†…ãƒãƒ£ãƒƒãƒˆ", page_icon="ğŸ’ƒ", layout="wide")
st.title("ğŸ’ƒ Multilingual Dance School Guide | å¤šè¨€èªãƒ€ãƒ³ã‚¹ã‚¹ã‚¯ãƒ¼ãƒ«æ¡ˆå†…")
st.caption("Ask me anything about dance schools in any language! | ã©ã®è¨€èªã§ã‚‚ãƒ€ãƒ³ã‚¹ã‚¹ã‚¯ãƒ¼ãƒ«ã«ã¤ã„ã¦èã„ã¦ãã ã•ã„ï¼ğŸŒâœ¨")

# ==== CSVèª­ã¿è¾¼ã¿ ==================================================
@st.cache_data(show_spinner=False)
def load_csv(path: str, encoding: str) -> pd.DataFrame:
    return pd.read_csv(path, encoding=encoding)

try:
    df = load_csv(CSV_FILE, ENCODING)
except Exception as e:
    st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

missing = [c for c in INDEX_COLS if c not in df.columns]
if missing:
    st.error(f"CSVã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing}")
    st.stop()

df[INDEX_COLS] = df[INDEX_COLS].fillna("")

# ==== ãƒãƒ£ãƒ³ã‚¯åŒ– ==================================================
SENT_SPLIT_RE = re.compile(r"(?<=[ã€‚ï¼.!?ï¼ï¼Ÿ])\s+|\n+")

def to_text(row: pd.Series) -> str:
    return " ".join(str(row[c]) for c in INDEX_COLS if c in row and str(row[c]).strip())

def split_into_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:
    sents = [s.strip() for s in SENT_SPLIT_RE.split(text) if s and s.strip()]
    chunks, buf = [], ""
    for s in sents:
        if len(buf) + len(s) <= chunk_size:
            buf = f"{buf} {s}".strip()
        else:
            if buf:
                chunks.append(buf)
            if overlap > 0 and len(buf) > overlap:
                buf = buf[-overlap:] + " " + s
            else:
                buf = s
    if buf:
        chunks.append(buf)
    return chunks

@st.cache_data(show_spinner=False)
def build_corpus(df: pd.DataFrame) -> List[Dict]:
    corpus = []
    for idx, row in df.iterrows():
        text = to_text(row)
        if not text:
            continue
        chunks = split_into_chunks(text, CHUNK_SIZE, CHUNK_OVERLAP)
        for j, ch in enumerate(chunks):
            corpus.append({"row_index": int(idx), "chunk_index": j, "text": ch})
    return corpus

corpus = build_corpus(df)

# ==== åŸ‹ã‚è¾¼ã¿ =====================================================
def hash_corpus(corpus: List[Dict]) -> str:
    h = hashlib.sha256()
    for item in corpus:
        h.update(item["text"].encode("utf-8"))
    return h.hexdigest()

@st.cache_resource(show_spinner=False)
def embed_corpus(corpus: List[Dict], model: str):
    texts = [c["text"] for c in corpus]
    resp = client.embeddings.create(model=model, input=texts)
    embeddings = np.array([d.embedding for d in resp.data], dtype=np.float32)
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
    return embeddings / norms

corpus_key = hash_corpus(corpus)
if "embeddings_key" not in st.session_state or st.session_state.embeddings_key != corpus_key:
    with st.spinner("è³‡æ–™ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¦ã„ã¾ã™â€¦"):
        EMB = embed_corpus(corpus, EMBED_MODEL)
    st.session_state.embeddings = EMB
    st.session_state.embeddings_key = corpus_key
else:
    EMB = st.session_state.embeddings

# ==== æ¤œç´¢ =========================================================
def embed_query(q: str) -> np.ndarray:
    resp = client.embeddings.create(model=EMBED_MODEL, input=[q])
    v = np.array(resp.data[0].embedding, dtype=np.float32)
    return v / (np.linalg.norm(v) + 1e-10)

def topk_search(q: str, k: int) -> List[Tuple[int, float]]:
    qv = embed_query(q)
    sims = (EMB @ qv)
    order = np.argsort(-sims)[:k]
    return [(int(i), float(sims[i])) for i in order]

def build_context(chosen: List[Tuple[int, float]], max_chars: int) -> str:
    parts, total = [], 0
    for idx, score in chosen:
        snippet = f"[{corpus[idx]['row_index']}-{corpus[idx]['chunk_index']} sim={score:.3f}]\n{corpus[idx]['text']}"
        if total + len(snippet) > max_chars:
            break
        parts.append(snippet)
        total += len(snippet)
    return "\n\n---\n\n".join(parts)

# ==== è¨€èªåˆ¤å®šã¨å¤šè¨€èªå¯¾å¿œ ==========================================
def detect_language_and_get_system_prompt(question: str) -> tuple[str, str]:
    """è³ªå•ã®è¨€èªã‚’åˆ¤å®šã—ã€é©åˆ‡ãªã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿”ã™"""
    
    # OpenAI APIã§è¨€èªåˆ¤å®š
    try:
        detection_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user", 
                "content": f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯è¨€èªã‚³ãƒ¼ãƒ‰ã®ã¿ï¼ˆä¾‹: ja, en, zh, ko, es, fr, de, etc.ï¼‰ã§è¿”ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {question}"""
            }],
            max_tokens=10,
            temperature=0.0
        )
        language_code = detection_response.choices[0].message.content.strip().lower()
    except:
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ—¥æœ¬èªã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹
        language_code = "ja"
    
    # è¨€èªåˆ¥ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã€Œåˆ†ã‹ã‚‰ãªã„ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    language_prompts = {
        "ja": {
            "system": "ã‚ãªãŸã¯å³æ ¼ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå›ç­”ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ ¹æ‹ ãŒç„¡ã„å ´åˆã¯ã€å¿…ãšã€åˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚",
            "no_answer": "åˆ†ã‹ã‚Šã¾ã›ã‚“"
        },
        "en": {
            "system": "You are a strict document-based assistant. Answer concisely in English based only on the given context. If there is no evidence in the context, you must answer only 'I don't know'.",
            "no_answer": "I don't know"
        },
        "zh": {
            "system": "æ‚¨æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ–‡æ¡£å›ç­”åŠ©æ‰‹ã€‚è¯·ä»…åŸºäºç»™å®šçš„ä¸Šä¸‹æ–‡ç”¨ä¸­æ–‡ç®€æ´å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æ ¹æ®ï¼Œæ‚¨å¿…é¡»åªå›ç­”'æˆ‘ä¸çŸ¥é“'ã€‚",
            "no_answer": "æˆ‘ä¸çŸ¥é“"
        },
        "ko": {
            "system": "ë‹¹ì‹ ì€ ì—„ê²©í•œ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê¸°ë°˜í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            "no_answer": "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"
        },
        "es": {
            "system": "Eres un asistente estricto basado en documentos. Responde de manera concisa en espaÃ±ol basÃ¡ndote solo en el contexto dado. Si no hay evidencia en el contexto, debes responder solo 'No lo sÃ©'.",
            "no_answer": "No lo sÃ©"
        },
        "fr": {
            "system": "Vous Ãªtes un assistant strict basÃ© sur des documents. RÃ©pondez de maniÃ¨re concise en franÃ§ais en vous basant uniquement sur le contexte donnÃ©. S'il n'y a pas de preuve dans le contexte, vous devez rÃ©pondre seulement 'Je ne sais pas'.",
            "no_answer": "Je ne sais pas"
        },
        "de": {
            "system": "Sie sind ein strenger dokumentenbasierter Assistent. Antworten Sie prÃ¤gnant auf Deutsch, basierend nur auf dem gegebenen Kontext. Wenn es keine Belege im Kontext gibt, mÃ¼ssen Sie nur 'Ich weiÃŸ es nicht' antworten.",
            "no_answer": "Ich weiÃŸ es nicht"
        }
    }
    
    # å¯¾å¿œã—ã¦ã„ã‚‹è¨€èªã®å ´åˆã¯ãã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã€ãã†ã§ãªã‘ã‚Œã°è‹±èªã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
    if language_code in language_prompts:
        selected = language_prompts[language_code]
    else:
        selected = language_prompts["en"]
    
    return language_code, selected["system"], selected["no_answer"]

# ==== LLMå‘¼ã³å‡ºã—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œãƒ»å¤šè¨€èªå¯¾å¿œï¼‰ ===================
def stream_llm_answer(question: str, context: str, history_pairs: List[Dict]) -> Generator[str, None, None]:
    # è¨€èªåˆ¤å®šã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—
    language_code, system_prompt, no_answer_message = detect_language_and_get_system_prompt(question)
    
    messages = [{"role": "system", "content": system_prompt}]
    for h in history_pairs[-USE_HISTORY_TURNS:]:
        messages.append({"role": "user", "content": h.get("user", "")})
        if h.get("assistant"):
            messages.append({"role": "assistant", "content": h["assistant"]})
    messages.append({
        "role": "user",
        "content": f"### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\n{context}\n\n### è³ªå•\n{question}\n\nâ€»æ ¹æ‹ ãŒç„¡ã‘ã‚Œã°ã€åˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚"
    })

    if not STREAM_ANSWER:
        resp = client.chat.completions.create(
            model=CHAT_MODEL, messages=messages, temperature=0.0, max_tokens=400
        )
        return resp.choices[0].message.content.strip()

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
    stream = client.chat.completions.create(
        model=CHAT_MODEL, messages=messages, temperature=0.0, max_tokens=400, stream=True
    )
    full = ""
    for chunk in stream:
        delta = chunk.choices[0].delta.content or ""
        full += delta
        yield delta  # å¢—åˆ†ã‚’è¿”ã™
    yield f"__FINAL__{full}"

# ==== ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ï¼ˆãƒªãƒ­ãƒ¼ãƒ‰ã§æ¶ˆãˆã‚‹ï¼‰ =============================
if "messages" not in st.session_state:
    st.session_state.messages = []  # [{"role": "...", "content": "...", "ts": "..."}]
if "pairs" not in st.session_state:
    st.session_state.pairs = []     # [{"user": "...", "assistant": "..."}]
if "clear_input" not in st.session_state:
    st.session_state.clear_input = False  # å…¥åŠ›æ¬„ã‚¯ãƒªã‚¢ãƒ•ãƒ©ã‚°

# ==== ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤ºï¼ˆLINEé¢¨ï¼‰ ====================================
st.markdown("---")
st.subheader("ğŸ’¬ Chat | ãƒãƒ£ãƒƒãƒˆ")

# ã‚«ã‚¹ã‚¿ãƒ CSSï¼ˆLINEé¢¨ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°ï¼‰
st.markdown("""
<style>
.user-message {
    background-color: #007AFF;
    color: white;
    padding: 10px 15px;
    border-radius: 20px;
    margin: 10px 0;
    margin-left: 20%;
    text-align: right;
    max-width: 70%;
    float: right;
    clear: both;
}
.assistant-message {
    background-color: #E5E5EA;
    color: black;
    padding: 10px 15px;
    border-radius: 20px;
    margin: 10px 0;
    margin-right: 20%;
    text-align: left;
    max-width: 70%;
    float: left;
    clear: both;
}
.message-spacer {
    height: 10px;
    clear: both;
}
</style>
""", unsafe_allow_html=True)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤ºã‚¨ãƒªã‚¢
if st.session_state.messages:
    # æ—¢å­˜ã®å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºï¼ˆå¤ã„ã‚‚ã®ã‹ã‚‰é †ã«ï¼‰
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.markdown(f'''
            <div class="user-message">
                {msg["content"]}
            </div>
            <div class="message-spacer"></div>
            ''', unsafe_allow_html=True)
        else:
            st.markdown(f'''
            <div class="assistant-message">
                {msg["content"]}
            </div>
            <div class="message-spacer"></div>
            ''', unsafe_allow_html=True)
else:
    # åˆå›è¡¨ç¤ºæ™‚ã®ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    st.info("ğŸ‘‹ Hello! I'd love to hear anything about dance schools. | ã“ã‚“ã«ã¡ã¯ï¼ãƒ€ãƒ³ã‚¹ã‚¹ã‚¯ãƒ¼ãƒ«ã«ã¤ã„ã¦ä½•ã§ã‚‚ãŠèããã ã•ã„ã€‚")

# å…¥åŠ›æ¬„ï¼ˆä¸‹éƒ¨å›ºå®šé¢¨ï¼‰
st.markdown("---")
col1, col2 = st.columns([5, 1])
with col1:
    # å…¥åŠ›æ¬„ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŸã‚ã«ã€clear_inputãƒ•ãƒ©ã‚°ã«ã‚ˆã£ã¦å‹•çš„ã«keyã‚’å¤‰æ›´
    input_key = f"message_input_{len(st.session_state.messages)}"
    user_input = st.text_input("ğŸ’­ Enter your message | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›", value="", placeholder="Please feel free to ask me anything about dance schools... | ãƒ€ãƒ³ã‚¹ã‚¹ã‚¯ãƒ¼ãƒ«ã«ã¤ã„ã¦ä½•ã§ã‚‚èã„ã¦ãã ã•ã„...", key=input_key)
with col2:
    st.write(""); st.write("")
    ask_clicked = st.button("â¤", help="é€ä¿¡")

# ==== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ==================================================
def answer_pipeline(question: str) -> str:
    """è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰"""
    # è¨€èªåˆ¤å®š
    language_code, system_prompt, no_answer_message = detect_language_and_get_system_prompt(question)
    
    top = topk_search(question, TOP_K)
    if not top or top[0][1] < MIN_TOP_SIM:
        return no_answer_message
    
    context = build_context(top, MAX_CONTEXT_CHARS)
    
    # LLMã«å›ç­”ç”Ÿæˆã‚’ä¾é ¼
    try:
        messages = [
            {"role": "system", "content": f"{system_prompt}\n\nå‚è€ƒè³‡æ–™:\n{context}"},
            {"role": "user", "content": question}
        ]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=500,
            temperature=0.3
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚è¨€èªã«å¿œã˜ã¦å¤‰æ›´
        error_messages = {
            "ja": f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ğŸ˜…\n{str(e)}",
            "en": f"An error occurred while generating the answer. ğŸ˜…\n{str(e)}",
            "zh": f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ã€‚ğŸ˜…\n{str(e)}",
            "ko": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜…\n{str(e)}",
            "es": f"OcurriÃ³ un error al generar la respuesta. ğŸ˜…\n{str(e)}",
            "fr": f"Une erreur s'est produite lors de la gÃ©nÃ©ration de la rÃ©ponse. ğŸ˜…\n{str(e)}",
            "de": f"Ein Fehler ist bei der Antwortgenerierung aufgetreten. ğŸ˜…\n{str(e)}"
        }
        return error_messages.get(language_code, error_messages["en"])

# ==== å®Ÿè¡Œï¼ˆå±¥æ­´ã‚’æ®‹ã™ãƒ»ãƒªãƒ­ãƒ¼ãƒ‰ã§ã‚¯ãƒªã‚¢ï¼‰ =========================
if ask_clicked and user_input.strip():
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": user_input})

    # å›ç­”ç”Ÿæˆä¸­ã®è¡¨ç¤º
    with st.spinner("ğŸ’­ å›ç­”ã‚’è€ƒãˆã¦ã„ã¾ã™..."):
        answer = answer_pipeline(user_input)

    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "assistant", "content": answer})
    st.session_state.pairs.append({"user": user_input, "assistant": answer})
    
    # ãƒšãƒ¼ã‚¸ã‚’å†å®Ÿè¡Œã—ã¦æœ€æ–°ã®å±¥æ­´ã‚’è¡¨ç¤ºï¼ˆå…¥åŠ›æ¬„ã¯æ–°ã—ã„keyã§è‡ªå‹•çš„ã«ã‚¯ãƒªã‚¢ã•ã‚Œã‚‹ï¼‰
    st.rerun()

# ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
if st.button("New Project | æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã«ã™ã‚‹"):
    st.session_state.messages = []
    st.session_state.pairs = []
    st.rerun()