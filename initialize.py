import os
import hashlib
from typing import List, Dict

import numpy as np
import pandas as pd
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv

import constants as C
import utils as U


# .envèª­ã¿è¾¼ã¿ã¯åˆå›ã«ä¸€åº¦
load_dotenv()


def create_client() -> OpenAI:
    return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def set_page():
    st.set_page_config(
        page_title="Dance School Guide Chat | ãƒ€ãƒ³ã‚¹ã‚¹ã‚¯ãƒ¼ãƒ«æ¡ˆå†…ãƒãƒ£ãƒƒãƒˆ",
        page_icon="ğŸ’ƒ",
        layout="wide"
    )


# ==== CSVèª­ã¿è¾¼ã¿ ==================================================
@st.cache_data(show_spinner=False)
def load_csv(path: str, encoding: str) -> pd.DataFrame:
    df = pd.read_csv(path, encoding=encoding)
    missing = [c for c in C.INDEX_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"CSVã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing}")
    df[C.INDEX_COLS] = df[C.INDEX_COLS].fillna("")
    return df


# ==== ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ï¼ˆãƒãƒ£ãƒ³ã‚¯åŒ–ï¼‰ ====================================
@st.cache_data(show_spinner=False)
def build_corpus(df: pd.DataFrame) -> List[Dict]:
    corpus: List[Dict] = []
    for idx, row in df.iterrows():
        text = U.to_text(row)
        if not text:
            continue
        chunks = U.split_into_chunks(text, C.CHUNK_SIZE, C.CHUNK_OVERLAP)
        for j, ch in enumerate(chunks):
            corpus.append({"row_index": int(idx), "chunk_index": j, "text": ch})
    return corpus


def _hash_corpus(corpus: List[Dict]) -> str:
    h = hashlib.sha256()
    for item in corpus:
        h.update(item["text"].encode("utf-8"))
    return h.hexdigest()


# ==== åŸ‹ã‚è¾¼ã¿ï¼ˆç”Ÿæˆï¼†ä¿æŒï¼‰ =======================================
@st.cache_resource(show_spinner=False)
def _embed_corpus_cached(texts: List[str], model: str):
    """
    clientã‚’å¼•æ•°ã«å«ã‚ãªã„ã“ã¨ã§cacheã®ã‚­ãƒ¼ãŒå®‰å®šã€‚
    å†…éƒ¨ã§OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’éƒ½åº¦ç”Ÿæˆï¼ˆç’°å¢ƒå¤‰æ•°ä½¿ç”¨ï¼‰ã€‚
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    resp = client.embeddings.create(model=model, input=texts)
    embeddings = np.array([d.embedding for d in resp.data], dtype=np.float32)
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
    return embeddings / norms


def ensure_embeddings(corpus: List[Dict]):
    corpus_key = _hash_corpus(corpus)
    if "embeddings_key" not in st.session_state or st.session_state.embeddings_key != corpus_key:
        with st.spinner("è³‡æ–™ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¦ã„ã¾ã™â€¦"):
            texts = [c["text"] for c in corpus]
            EMB = _embed_corpus_cached(texts, C.EMBED_MODEL)
        st.session_state.embeddings = EMB
        st.session_state.embeddings_key = corpus_key
    return st.session_state.embeddings


# ==== ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ– =============================================
def ensure_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []  # [{"role": "...", "content": "..."}]
    if "pairs" not in st.session_state:
        st.session_state.pairs = []     # [{"user": "...", "assistant": "..."}]
    if "clear_input" not in st.session_state:
        st.session_state.clear_input = False