from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
from openai import OpenAI

import constants as C


# ==== åŸºæœ¬ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† =============================================
def to_text(row: pd.Series) -> str:
    return " ".join(str(row[c]) for c in C.INDEX_COLS if c in row and str(row[c]).strip())


def split_into_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:
    sents = [s.strip() for s in C.SENT_SPLIT_RE.split(text) if s and s.strip()]
    chunks, buf = [], ""
    for s in sents:
        if len(buf) + len(s) <= chunk_size:
            buf = f"{buf} {s}".strip()
        else:
            if buf:
                chunks.append(buf)
            if overlap > 0 and len(buf) > overlap:
                buf = buf[-overlap:] + " " + s
            else:
                buf = s
    if buf:
        chunks.append(buf)
    return chunks


# ==== åŸ‹ã‚è¾¼ã¿ã‚¯ã‚¨ãƒªï¼†æ¤œç´¢ ==========================================
def embed_query(q: str, client: OpenAI) -> np.ndarray:
    resp = client.embeddings.create(model=C.EMBED_MODEL, input=[q])
    v = np.array(resp.data[0].embedding, dtype=np.float32)
    return v / (np.linalg.norm(v) + 1e-10)


def topk_search(q: str, EMB: np.ndarray, client: OpenAI, k: int) -> List[Tuple[int, float]]:
    qv = embed_query(q, client)
    sims = (EMB @ qv)
    order = np.argsort(-sims)[:k]
    return [(int(i), float(sims[i])) for i in order]


def build_context(chosen: List[Tuple[int, float]], corpus: List[Dict], max_chars: int) -> str:
    parts, total = [], 0
    for idx, score in chosen:
        snippet = f"[{corpus[idx]['row_index']}-{corpus[idx]['chunk_index']} sim={score:.3f}]\n{corpus[idx]['text']}"
        if total + len(snippet) > max_chars:
            break
        parts.append(snippet)
        total += len(snippet)
    return "\n\n---\n\n".join(parts)


# ==== è¨€èªåˆ¤å®šï¼†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ =========================================
def detect_language_and_get_system_prompt(question: str, client: OpenAI) -> tuple[str, str, str]:
    """è³ªå•ã®è¨€èªã‚’åˆ¤å®šã—ã€(language_code, system_prompt, no_answer_msg) ã‚’è¿”ã™"""
    language_prompts = {
        "ja": {
            "system": "ã‚ãªãŸã¯å³æ ¼ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå›ç­”ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ ¹æ‹ ãŒç„¡ã„å ´åˆã¯ã€å¿…ãšã€åˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚",
            "no_answer": "åˆ†ã‹ã‚Šã¾ã›ã‚“"
        },
        "en": {
            "system": "You are a strict document-based assistant. Answer concisely in English based only on the given context. If there is no evidence in the context, you must answer only 'I don't know'.",
            "no_answer": "I don't know"
        },
        "zh": {
            "system": "æ‚¨æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ–‡æ¡£å›ç­”åŠ©æ‰‹ã€‚è¯·ä»…åŸºäºç»™å®šçš„ä¸Šä¸‹æ–‡ç”¨ä¸­æ–‡ç®€æ´å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰æ ¹æ®ï¼Œæ‚¨å¿…é¡»åªå›ç­”'æˆ‘ä¸çŸ¥é“'ã€‚",
            "no_answer": "æˆ‘ä¸çŸ¥é“"
        },
        "ko": {
            "system": "ë‹¹ì‹ ì€ ì—„ê²©í•œ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê¸°ë°˜í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            "no_answer": "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"
        },
        "es": {
            "system": "Eres un asistente estricto basado en documentos. Responde de manera concisa en espaÃ±ol basÃ¡ndote solo en el contexto dado. Si no hay evidencia en el contexto, debes responder solo 'No lo sÃ©'.",
            "no_answer": "No lo sÃ©"
        },
        "fr": {
            "system": "Vous Ãªtes un assistant strict basÃ© sur des documents. RÃ©pondez de maniÃ¨re concise en franÃ§ais en vous basant uniquement sur le contexte donnÃ©. S'il n'y a pas de preuve dans le contexte, vous devez rÃ©pondre seulement 'Je ne sais pas'.",
            "no_answer": "Je ne sais pas"
        },
        "de": {
            "system": "Sie sind ein strenger dokumentenbasierter Assistent. Antworten Sie prÃ¤gnant auf Deutsch, basierend nur auf dem gegebenen Kontext. Wenn es keine Belege im Kontext gibt, mÃ¼ssen Sie nur 'Ich weiÃŸ es nicht' antworten.",
            "no_answer": "Ich weiÃŸ es nicht"
        }
    }

    # ãƒ¢ãƒ‡ãƒ«ã§è¨€èªåˆ¤å®šï¼ˆä¾‹å¤–æ™‚ã¯jaï¼‰
    try:
        detection_response = client.chat.completions.create(
            model=C.DETECT_MODEL,
            messages=[{
                "role": "user",
                "content": (
                    "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯è¨€èªã‚³ãƒ¼ãƒ‰ã®ã¿ï¼ˆä¾‹: ja, en, zh, ko, es, fr, de, etc.ï¼‰ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
                    f"ãƒ†ã‚­ã‚¹ãƒˆ: {question}"
                )
            }],
            max_tokens=10,
            temperature=0.0
        )
        language_code = detection_response.choices[0].message.content.strip().lower()
    except Exception:
        language_code = "ja"

    selected = language_prompts.get(language_code, language_prompts["en"])
    return language_code, selected["system"], selected["no_answer"]


# ==== å›ç­”ç”Ÿæˆï¼ˆRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰ ===================================
def answer_pipeline(
    question: str,
    corpus: List[Dict],
    EMB: np.ndarray,
    client: OpenAI
) -> str:
    # è¨€èªåˆ¤å®š
    language_code, system_prompt, no_answer_message = detect_language_and_get_system_prompt(question, client)

    # è¿‘å‚æ¤œç´¢
    top = topk_search(question, EMB, client, C.TOP_K)
    if not top or top[0][1] < C.MIN_TOP_SIM:
        return no_answer_message

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆçµ„ã¿ç«‹ã¦
    context = build_context(top, corpus, C.MAX_CONTEXT_CHARS)

    # LLMã«å›ç­”ç”Ÿæˆ
    try:
        messages = [
            {"role": "system", "content": f"{system_prompt}\n\nå‚è€ƒè³‡æ–™:\n{context}"},
            {"role": "user", "content": question}
        ]
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",  # å…ƒã‚³ãƒ¼ãƒ‰æº–æ‹ ï¼ˆCHAT_MODELã¨ã¯åˆ¥ã§é‹ç”¨ï¼‰
            messages=messages,
            max_tokens=500,
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        # è¨€èªåˆ¥ã‚¨ãƒ©ãƒ¼æ–‡
        error_messages = {
            "ja": f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ğŸ˜…\n{str(e)}",
            "en": f"An error occurred while generating the answer. ğŸ˜…\n{str(e)}",
            "zh": f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ã€‚ğŸ˜…\n{str(e)}",
            "ko": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ã€‚ğŸ˜…\n{str(e)}",
            "es": f"OcurriÃ³ un error al generar la respuestaã€‚ğŸ˜…\n{str(e)}",
            "fr": f"Une erreur s'est produite lors de la gÃ©nÃ©ration de la rÃ©ponseã€‚ğŸ˜…\n{str(e)}",
            "de": f"Ein Fehler ist bei der Antwortgenerierung aufgetretenã€‚ğŸ˜…\n{str(e)}"
        }
        return error_messages.get(language_code, error_messages["en"])